{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'title', 'content'],\n",
       "        num_rows: 560000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'title', 'content'],\n",
       "        num_rows: 70000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"dbpedia_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'title', 'content'],\n",
       "    num_rows: 560000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"dbpedia_14\", split=\"train\")\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'title': 'E. D. Abbott Ltd',\n",
       " 'content': ' Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TO_CATEGORY = {\n",
    "    0: \"Company\",\n",
    "    1: \"Educational Institution\",\n",
    "    2: \"Artist\",\n",
    "    3: \"Athlete\",\n",
    "    4: \"Office Holder\",\n",
    "    5: \"Mean Of Transportation\",\n",
    "    6: \"Building\",\n",
    "    7: \"Natural Place\",\n",
    "    8: \"Village\",\n",
    "    9: \"Animal\",\n",
    "    10: \"Plant\",\n",
    "    11: \"Album\",\n",
    "    12: \"Film\",\n",
    "    13: \"Written Work\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_samples = {}\n",
    "for sample in dataset_train:\n",
    "    category = LABEL_TO_CATEGORY[sample[\"label\"]]\n",
    "    if category not in category_to_samples:\n",
    "        category_to_samples[category] = []\n",
    "    category_to_samples[category].append(sample[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_negatives = {\n",
    "    category: [desc for cat, descriptions in category_to_samples.items() if cat != category for desc in descriptions]\n",
    "    for category in LABEL_TO_CATEGORY.values()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example, num_negatives=5):\n",
    "    category = LABEL_TO_CATEGORY.get(example[\"label\"], None)\n",
    "    \n",
    "    if category is None or category not in category_negatives:\n",
    "        return None  \n",
    "\n",
    "    query = f\"Tell me about {category.lower()}.\"\n",
    "    positive = example[\"content\"]\n",
    "\n",
    "    negatives = random.choices(category_negatives[category], k=num_negatives)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_train = dataset_train.map(preprocess, remove_columns=dataset_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_train = processed_data_train.filter(lambda x: x['query'] is not None and x['positive'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'positive', 'negatives'],\n",
       "    num_rows: 560000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me about company.',\n",
       " 'positive': ' Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.',\n",
       " 'negatives': [' The Ford Model 15-P flying wing was the last aircraft developed by the Stout Metal Airplane Division of the Ford Motor Company. After several flights resulting in a crash the program was halted. Ford eventually re-entered the aviation market producing Consolidated B-24 Liberators under license from Consolidated Aircraft.',\n",
       "  ' Nellie Stockbridge (ca. 1868 – May 22 1965) was an early Idaho frontier mining district photographer. Her career spanned over 60 years. She was the oldest living member of the Zonta International club for advancement of women when she died in 1965.',\n",
       "  ' Shurikeh (Persian: شوريكه\\u200e also Romanized as Shūrīḵeh) is a village in Darbqazi Rural District in the Central District of Nishapur County Razavi Khorasan Province Iran. At the 2006 census its existence was noted but its population was not reported.',\n",
       "  \" Walker Valley High School (WVHS) is a public high school in the Bradley County Schools system located in the northern part of Bradley County Tennessee near Charleston Tennessee. The school serves about 1600 students in grades 9–12.The school's mascot is the mustang and its school colors are blue and gold. Walker Valley has been in existence since 2001. The current principal is Mr. Danny Coggin.\",\n",
       "  ' Into the Unknown is the fifth album by Mercyful Fate released by Metal Blade Records in 1996.Into the Unknown is the most commercially successful Mercyful Fate album to date. It peaked at #31 in the Finnish charts remaining for two weeks in the Top 40. It is the only album by the band to appear on the charts.']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_pairs_train = []\n",
    "for item in processed_data_train:\n",
    "    query = item[\"query\"]\n",
    "    positive = item[\"positive\"]\n",
    "    negatives = item[\"negatives\"]\n",
    "    contrastive_pairs_train.append({\n",
    "        \"anchor\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': 'Tell me about film.',\n",
       " 'positive': ' Rojo Amanecer (Red Dawn) is a 1989 Silver Ariel Award-winning Mexican film directed by Jorge Fons. It is a film about the Tlatelolco Massacre in the section of Tlatelolco in Mexico City in the evening of October 2 1968. It focuses on the day of a middle-class Mexican family living in one of the apartment buildings surrounding the Plaza de Tlatelolco and is based on testimonials from witnesses and victims. It stars Héctor Bonilla María Rojo the Bichir Brothers Eduardo Palomo and others.',\n",
       " 'negatives': [' Karschiola is a genus of moths in the family Arctiidae. It contains the single species Karschiola holoclera which is found in Malawi Tanzania and Zimbabwe.',\n",
       "  ' Billie Fulford (21 August 1914 – 28 May 1987) was a New Zealand cricketer. She played in one Test match in 1948.',\n",
       "  ' Discovery or Discoverie was a small 20-ton 38 foot (12 m) long fly-boat of the British East India Company launched before 1602. The ship was one of three that participated in the voyage that led to the founding of Jamestown Virginia.',\n",
       "  ' Hemidoras stenopeltis is a species of thorny catfish found in the Amazon basin of Brazil Colombia and Peru. This species grows to a length of 12.5 centimetres (4.9 in) SL.',\n",
       "  ' Maxilua is a genus of moths of the Noctuidae family.']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_pairs_train[500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contrastive_pairs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDataset:\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.pairs[idx]\n",
    "        return item[\"anchor\"], item[\"positive\"], item[\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_dataset_train = ContrastiveDataset(contrastive_pairs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(contrastive_dataset_train, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17500"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type= \"FEATURE_EXTRACTION\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x, y):\n",
    "    return 1 - torch.nn.functional.cosine_similarity(x, y, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(anchor_embedding, positive_embedding, negative_embedding, distance_fn):\n",
    "\n",
    "    pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "    neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "    \n",
    "    logits = torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'title', 'content'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val = load_dataset(\"dbpedia_14\", split=\"test\")\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_samples_val = {}\n",
    "for sample in dataset_val:\n",
    "    category = LABEL_TO_CATEGORY[sample[\"label\"]]\n",
    "    if category not in category_to_samples_val:\n",
    "        category_to_samples_val[category] = []\n",
    "    category_to_samples_val[category].append(sample[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_negatives_val = {\n",
    "    category: [desc for cat, descriptions in category_to_samples_val.items() if cat != category for desc in descriptions]\n",
    "    for category in LABEL_TO_CATEGORY.values()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_val(example, num_negatives=5):\n",
    "    category = LABEL_TO_CATEGORY.get(example[\"label\"], None)\n",
    "    \n",
    "    if category is None or category not in category_negatives_val:\n",
    "        return None\n",
    "\n",
    "    query = f\"Tell me about {category.lower()}.\"\n",
    "    positive = example[\"content\"]\n",
    "\n",
    "    negatives = random.choices(category_negatives_val[category], k=num_negatives)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_val = dataset_val.map(preprocess_val, remove_columns=dataset_val.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data_val = processed_data_val.filter(lambda x: x['query'] is not None and x['positive'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_pairs_val = []\n",
    "for item in processed_data_val:\n",
    "    query = item[\"query\"]\n",
    "    positive = item[\"positive\"]\n",
    "    negatives = item[\"negatives\"]\n",
    "    contrastive_pairs_val.append({\n",
    "        \"anchor\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_dataset_val = ContrastiveDataset(contrastive_pairs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val = DataLoader(contrastive_dataset_val, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2188"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_embedding(inputs, model):\n",
    "    input=model(**inputs)\n",
    "    input_last_hidden_state=input.last_hidden_state\n",
    "    input_attention_mask = inputs['attention_mask']\n",
    "    input_masked_embeddings = input_last_hidden_state * input_attention_mask.unsqueeze(-1)\n",
    "    input_sum_embeddings = torch.sum(input_masked_embeddings, dim=1)\n",
    "    input_token_counts = torch.sum(input_attention_mask, dim=1).unsqueeze(-1)\n",
    "    input_avg_embeddings = input_sum_embeddings / input_token_counts\n",
    "    return(input_avg_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mrr(model, data_loader_val, distance_fn):\n",
    "    model.eval()  \n",
    "\n",
    "    total_rr = 0.0\n",
    "    num_queries = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader_val:\n",
    "            anchor_text = batch[0]\n",
    "            positive_text = batch[1]\n",
    "            negative_texts = batch[2]\n",
    "\n",
    "            anchor_input = tokenizer(anchor_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            positive_input = tokenizer(positive_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "            anchor_embedding = avg_embedding(anchor_input, model)\n",
    "            positive_embedding = avg_embedding(positive_input, model)\n",
    "            negative_embedding = [avg_embedding(tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device), model) for neg in negative_texts]\n",
    "\n",
    "            pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "            neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "            all_similarities=torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "            \n",
    "            sorted_similarities, sorted_indices = torch.sort(all_similarities, dim=1, descending=True)\n",
    "\n",
    "            # Find the rank of the first relevant (positive) document\n",
    "            positive_rank = (sorted_indices == 0).nonzero(as_tuple=True)[1] + 1  # +1 to make rank 1-based\n",
    "            total_rr += torch.sum(1.0 / positive_rank.float()).item()  # Reciprocal rank\n",
    "            num_queries += len(positive_rank)\n",
    "\n",
    "    mrr = total_rr / num_queries\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir =\"/dss/dsshome1/07/ra65bex2/srawat/wiki/average\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epoch_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "Checkpoint saved: /dss/dsshome1/07/ra65bex2/srawat/wiki/baseline/checkpoint_epoch_1.pth\n",
      "Epoch 1/3, Training Loss: 1.6659837365150452\n",
      "Mean Reciprocal Rank (MRR) for validation set: 0.5917\n",
      "Epoch 1 took 0.1394 minutes.\n",
      "\n",
      "\n",
      "EPOCH 2:\n",
      "Checkpoint saved: /dss/dsshome1/07/ra65bex2/srawat/wiki/baseline/checkpoint_epoch_2.pth\n",
      "Epoch 2/3, Training Loss: 1.712405264377594\n",
      "Mean Reciprocal Rank (MRR) for validation set: 0.5917\n",
      "Epoch 2 took 0.0358 minutes.\n",
      "\n",
      "\n",
      "EPOCH 3:\n",
      "Checkpoint saved: /dss/dsshome1/07/ra65bex2/srawat/wiki/baseline/checkpoint_epoch_3.pth\n",
      "Epoch 3/3, Training Loss: 1.66230309009552\n",
      "Mean Reciprocal Rank (MRR) for validation set: 0.5917\n",
      "Epoch 3 took 0.0420 minutes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    lora_model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader_train:\n",
    "        \n",
    "        anchor_texts = batch[0]\n",
    "        positive_texts = batch[1]\n",
    "        negative_texts = batch[2]\n",
    "     \n",
    "        anchor_inputs = tokenizer(anchor_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        positive_inputs = tokenizer(positive_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "    \n",
    "        anchor_embedding = avg_embedding(anchor_inputs, lora_model)\n",
    "        positive_embedding = avg_embedding(positive_inputs, lora_model)\n",
    "        negative_embedding = [avg_embedding(tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device), lora_model) for neg in negative_texts]\n",
    "\n",
    "        loss = info_nce_loss(anchor_embedding, positive_embedding, negative_embedding, distance_fn=cosine_distance)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    save_path = os.path.join(save_dir, f\"average_checkpoint_epoch_{epoch+1}.pth\")\n",
    "    torch.save(lora_model, save_path)\n",
    "    print(f\"EPOCH {epoch+1}:\")\n",
    "    print(f\"Checkpoint saved: {save_path}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss / len(data_loader_train)}\")\n",
    "    mrr_validation = evaluate_mrr(lora_model, data_loader_val, cosine_distance)\n",
    "    print(f\"Mean Reciprocal Rank (MRR) for validation set: {mrr_validation:.4f}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1} took {(end_time - start_time) / 60:.4f} minutes.\")\n",
    "    print(f\"\\n\")\n",
    "    epoch_metrics.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'training_loss': total_loss / len(data_loader_train),\n",
    "        'mrr_validation': mrr_validation,\n",
    "        'time_taken_minutes': (end_time - start_time) / 60\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_dir + '/epoch_metrics.json', 'w') as f:\n",
    "    json.dump(epoch_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
