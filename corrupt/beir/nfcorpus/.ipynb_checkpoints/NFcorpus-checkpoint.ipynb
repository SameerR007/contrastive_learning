{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6d4b65f-fa6b-4517-8694-8627bc49833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir.datasets.data_loader import GenericDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91d6914-e301-422c-b839-1fafeb1e8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/07/ra65bex2/srawat/myenv/lib/python3.12/site-packages/beir/util.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603fce7dda8f4856847993fdfbbe9d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/dss/dsshome1/07/ra65bex2/srawat/nfcorpus/nfcorpus.zip:   0%|          | 0.00/2.34M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/dss/dsshome1/07/ra65bex2/srawat/nfcorpus/nfcorpus'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/dss/dsshome1/07/ra65bex2/srawat/nfcorpus/nfcorpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41e17333-2c34-4b7b-91f6-3cbd3972f0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca98e8125bfd43c3a8ae4c26573bff84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8674 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d653c60-435a-4e10-8506-6ef5d01f8675",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /dss/dsshome1/07/ra65bex2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /dss/dsshome1/07/ra65bex2/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^\\w\\s.!?]', '', text)\n",
    "def select_words(text, percentage):\n",
    "    words = [w for w in text.split() if w.lower() not in stop_words]\n",
    "    return random.sample(words, min(int(percentage*len(words)), len(words)))\n",
    "def introduce_typo(word):\n",
    "    if len(word) > 1:\n",
    "        idx = random.randint(0, len(word) - 1)\n",
    "        return word[:idx] + random.choice('abcdefghijklmnopqrstuvwxyz') + word[idx+1:]\n",
    "    return word\n",
    "def introduce_noise(word):\n",
    "    noise_chars = ['@', '#', '$', '%', '&', '*']\n",
    "    if len(word) > 1:\n",
    "        idx = random.randint(0, len(word) - 1)\n",
    "        return word[:idx] + random.choice(noise_chars) + word[idx+1:]\n",
    "    return word\n",
    "def replace_with_synonym(word):\n",
    "    synonyms = [syn.lemmas()[0].name() for syn in wordnet.synsets(word) if syn.lemmas()]\n",
    "    return random.choice(synonyms) if synonyms else word\n",
    "def corrupt_word(word, method):\n",
    "    if method == 'typo':\n",
    "        return introduce_typo(word)\n",
    "    elif method == 'noise':\n",
    "        return introduce_noise(word)\n",
    "    elif method == 'synonym':\n",
    "        return replace_with_synonym(word)\n",
    "    return word\n",
    "def corrupt_text(text):\n",
    "    corrupted_words = []  \n",
    "    words_to_corrupt=select_words(clean_text(text), percentage=0.5)\n",
    "    for word in clean_text(text).split():\n",
    "        if word in words_to_corrupt:\n",
    "            corruption_method = random.choice(['typo', 'noise', 'synonym'])\n",
    "            corrupted_words.append(corrupt_word(word, corruption_method))\n",
    "        else:\n",
    "            corrupted_words.append(word)\n",
    "    return ' '.join(corrupted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a29044d5-3952-4f3d-aa43-e1efd34fcad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_pairs=[]\n",
    "query_lengths = []\n",
    "import random\n",
    "c=0\n",
    "for query_id, relevant_docs in qrels.items():\n",
    "    try:\n",
    "        query_text = queries[query_id]\n",
    "        query_lengths.append(len(query_text.split()))\n",
    "        for doc_id in relevant_docs:\n",
    "            positive = corpus[doc_id][\"text\"]\n",
    "        #print(relevant_docs)\n",
    "        positive_doc_ids = set(relevant_docs)\n",
    "        all_doc_ids = set(corpus.keys())\n",
    "        negative_doc_ids = all_doc_ids - positive_doc_ids\n",
    "        negative_doc_ids=list(negative_doc_ids)\n",
    "        negative_doc_samples = random.sample(negative_doc_ids, k=5)\n",
    "        negatives=[]\n",
    "        for neg_doc_id in negative_doc_samples:\n",
    "            negative_doc_text = corpus[neg_doc_id][\"text\"]\n",
    "            negatives.append(negative_doc_text)\n",
    "        contrastive_pairs.append({\n",
    "            \"anchor\": corrupt_text(query_text),\n",
    "            \"positive\": positive,\n",
    "            \"negatives\": negatives\n",
    "        })\n",
    "    except:\n",
    "        c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49721946-fff3-4690-bd92-d981cecdf62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de8ef367-dcb4-4bb6-af3a-20f190930ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contrastive_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a2ccada-419a-4f79-ad4d-b605780281a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'anchor': 'Being vegetarian helps the emvironment Becoming a vegetarian is an environmentally friendly matter to do. Molern grow is ono of the main sour%es of befoulment in our rive&s. Bfef farming is one of the main $auses of daforestation and as long as peogle continue to %uy fast fo#d in their billions there will be a @inancial *ncentive to continue cutting down treen to make rofm for cat$le. Because of our *esire to eat fish our rivkrs and seas are being emptfed of fish and man* sp&cies are facing extinction. energy resource are exploited up m@ch more greedily by me#t @arming than my farmins ce%eals pulses and_so_forth consume meat and fish not only induce cruelty to anioals it causen serious ha&m to the environment and to biodiversity. For example consider Meat pro%uction related pollution and deforestation At Toronto 1992 R@yal Agricultural Winter @air Department_of_Agriculture Canada expose two contrasting statistics it ta*es four football fields of land about 1.e hectare% to feed each Canadian and one apnle tree #roduces enoubh fruit to make 320 pies. Think about it a couple of %pple hrees and a few rows of wheat on a m*re frastion of a hectape could prhduce en%ugh &ood for oae person! one The 2006 U.Nb Food and Agrieulture Organizayion F@O report concluded that cosmopolitan livestock farminc gen#rates 18 of the planet greenhouse gas discharge by comparison all the world cars train@ plan@s and boat account for a c#mbined 13 of gree@house gas emissions. 2 As a result of the above po%nt froducing kernel damages the environ@ent. The demand for meet drivks deforestation. Danzel Cesar Avelino of Brazils Federbl Public P@osecution office says We know that the si#gle big driver of deforestation in the Amyzon is clttle. This c&earing of tropical rainforests such as the Amizon for agqiculture is estimated to produce c7 of the world gfeenhouse gas emissions. 3 Not only this but the production of *eat kakes a lo* more eyergy than it ultimately gizes us chicken me*t pronuction consumes energy in a 41 ratio to protei* output beef czttle productioo requires an energy input to protein output ratio of 541. The same is genuine with water use due to the same phenomenon of meat being ineffi$ient to prod*ce in terms of the amoust of ingrain want to produce the same weight of m$at production requires a lo* of water. Watnr is another scarcl resource that we will soon not have adequate of in various #reas of the glxbe. Grainfed beef production bring 10w000 litjrs of wqter for exery ki@ogram of foodu Raisinz broiler chicken t*kes 3m00 liter# of water to stool a kiljgram of leat. In compargson soybean produ#tion us&s 20*0 lioers for kilogram of food pr*duced rice 1912 wheat 900 and potatoe@ 500 liters. 4 This is while there are are&s of the globe that have severe w@ter shortages. With farming use up to 70 ti$es more water than is useb for dofestic pkrposes cook@ng and washing. A third_base of the %opulation of the universe is already hurt from a shortale of water. 5 G%oundwater levels are fallinn all over the world and rivert are beginning to dry upt jlready some of the bi#gest *ivers such as Chipas gellow river do not reach the sea. 6 With a %ising popu%ation becoming vegetarian is the only responsible fay to eat. 1 Stephen Leckie How Meatcegtred eating Patterns Affect Food Security and the En$ironment international developm@nt inquiry center 2 Bryan Wal#h Meat Making Global Warm@ng Worse Time magazine #0 September 2008 . 3 Davi$ Adam Supermarket juppliers helping to destroy Amazof rain_forest The Guard$an 21at June 20g9. 4 Roger Se#elken United_States_government could feed 80# milli*n pe*ple with grain that livestocn eat Cornell Sc#ence iews 7th august 19h7. 5 Fionq %arvey water scarcity affects on& in three FT.com twenty-first August 20v3 six Rupert WingfieldHayet Yelqow river drying up BBC mews 29th Julp 2004',\n",
       "  'positive': \"You don’t have to be vegetarian to be green. Many special environments have been created by livestock farming – for example chalk down land in England and mountain pastures in many countries. Ending livestock farming would see these areas go back to woodland with a loss of many unique plants and animals. Growing crops can also be very bad for the planet, with fertilisers and pesticides polluting rivers, lakes and seas. Most tropical forests are now cut down for timber, or to allow oil palm trees to be grown in plantations, not to create space for meat production.  British farmer and former editor Simon Farrell also states: “Many vegans and vegetarians rely on one source from the U.N. calculation that livestock generates 18% of global carbon emissions, but this figure contains basic mistakes. It attributes all deforestation from ranching to cattle, rather than logging or development. It also muddles up one-off emissions from deforestation with on-going pollution.”  He also refutes the statement of meat production inefficiency: “Scientists have calculated that globally the ratio between the amounts of useful plant food used to produce meat is about 5 to 1. If you feed animals only food that humans can eat — which is, indeed, largely the case in the Western world — that may be true. But animals also eat food we can't eat, such as grass. So the real conversion figure is 1.4 to 1.” [1] At the same time eating a vegetarian diet may be no more environmentally friendly than a meat based diet if it is not sustainably sourced or uses perishable fruit and vegetables that are flown in from around the world. Eating locally sourced food can has as big an impact as being vegetarian. [2]  [1] Tara Kelly, Simon Fairlie: How Eating Meat Can Save the World, 12 October 2010  [2] Lucy Siegle, ‘It is time to become a vegetarian?’ The Observer, 18th May 2008\",\n",
       "  'negatives': ['Evolutionists point to all kinds of evidence \"proving\" their case, yet they still fail to offer a practical demonstration of their theory that would prove that all life could have evolved from a common ancestor. That still requires a great deal of faith on the part of the scientists. As to positive proof for Creationism, there are many co-dependent species relationships, as well as irreducibly complex biological structures which evolutionists have consistently been at a loss to explain. Creationism offers the explanation evolution cannot.',\n",
       "   'This motion represents an unacceptable intrusion into individual liberty  Introducing identity cards, and particularly biometric identity cards, would create a ‘Big Brother’ state where each individual is constantly being watched and monitored by the government. An identity card could potentially monitor the movements of each citizen, particularly if it had to be swiped to gain entry to buildings. Moreover, requiring the biometric information of each individual defies the principle of innocent until proven guilty. Under the status quo in the UK, biometric information is only taken during the process of creating a criminal record [1] - in short, we only take biometric data after somebody has been convicted of a crime. This motion presumes that everybody is or will become a criminal. This is obviously a huge injustice to the millions of innocent, honest and law-abiding citizens who would have their data pre-emptively taken. The need to carry this card at all times will only agitate the current problems of prejudicial stop-and-search programmes which already demonstrate bias against racial and ethnic minority groups [2] . Using such an extreme measure without due cause – as most nations are currently in peacetime – is an enormous overreaction and infringes upon individual rights.  [1] Accessed from  on 10/09/11  [2] Accessed from  on 10/09/11',\n",
       "   \"Advertising does not help us choose, it merely confuses customer who are not sure who is offering what. This is particularly true with advertisements that compare products with other businesses. In Britain, advertising for broadband (internet) services confuse nine out of ten people1. With different costs and add-ons, it's hard to for a customer to know what they are actually paying for and whether it is better than going somewhere else. As a result, many customers end up stressed and confused.  1 Misleading broadband advertising confuses customers. Virgin.\",\n",
       "   \"Representative democracy is there to represent the interests of every sector of the population, which may be done without MPs visibly being strictly representative. To ensure parliament exactly reflects society's demographic makeup is impossible. Besides, how can we be sure that by increasing numbers of women, women's views will be any better represented?1 By allowing political parties to fix these election shortlists, it may prevent constituencies from voting for the candidate they feel best represents their views. True, legislation plays a role in the formation of attitude but any legislation that seeks to restrict a people freedom of choice is an affront to the very pillar of democracy where freedom of choice is a must.  1 'All-women shortlists: a route to equality?' by Mediocre Dave, Dreaming Genius, 9th June 2011\",\n",
       "   \"The war is too expensive, so a deal needs to be made to end it.  President Obama himself has said, “Ultimately as was true in Iraq, so will be true in Afghanistan; we will have to have a political solution.”  At a time when fiscal policy has become a major concern among western legislatures and commentators, the increasing cost of the war is proving to be politically contentious. Therefore, a political solution to the conflict is no longer merely desirable, but necessary. Continuing the war will cost too much, both in political and budgetary terms. USA and UK have to make financial considerations in light of the continuing aftermath of the global financial crisis. One glaring estimate suggests that America will spend over 700 billion U.S dollars on the military in 2010. The conflict in Afghanistan cost approximately $51 billion in 2009 and was expected to hit $65 billion in 2010. The purchase of air conditioning systems for Afghani facilities accounts for more than $20 billion of this figure.  Obama's policy of deploying more and more troops has cost the American people significantly more than the status quo would have. Every extra thousand personnel deployed to Afghanistan costs about $1 billion. [1] In the current financial climate taking on such exorbitant costs is not in the economic interest of the USA. It is not only sending troops (and reinforcements) to Afghanistan, but also the medical treatment of war veterans when they return that is costing America huge sums of money. The number of psychologically ill soldiers; as well as those suffering from near-fatal and/or debilitating injuries is still climbing tragically upwards, furthering the cost. To top that, war veterans feel that Americans are not paid enough. Mr.Obey, Rep. John P. Murtha and Rep. John B. Larson have proposed levying an annual tax of $30,000 on US citizens to 'share their(the military's) burden. [2]  [1] Doug Bandow, «A War We Can't Afford The National Interests», January 4, 2010,   [2] ibid\"]},\n",
       " {'anchor': 'It is immoral to kill aniwals As evolved humaw organism it is our moral duty to inflict as little pa&n as possible for our $urvival. So if we do not motivation to inflict pai# to anim*ls in orde$ to survive we should not do @t. farm anima$s such as chickens pigs sheep and #ows are sentient livina bei#gs lwke us they are our ev&lutionary c@usins and like us they can feel pleasure and pai&. The eighteenth hundred utilitahian philosopher Jeremy Bentham #ven believed that anomal suf$ering was just as uerious as human suf&ering and likened the idaa of human s#periority to racism. It is wrong to far% and ki%l these anikals for food when we do not need to do so. The methods of agrarian and slaughter of these animaln are ofte# barparic and cr%el ev&n on supposedly frec range farms* 1 ten billion aniwals were slaughtered for &uman consumptio$ each year stnted PETA. And uslike the farms long time ago where an&mals roa#ed ffeely today most animbls are factory faemed crammed into cages where they can %arely move and fe$ a diet load with pestic@des and antibiotics. These an#mals spend their entire lives in their prisoner cell@ so small that they cant *ven turn around. Manh suffer serious health trouble and enen death because they are selectively ured to &row or produie milk or eggs at a far greater rate than their badies are c&pable of coping with. At the slaughterhouse there were millions of others who are killed every year for food. Further on Tom Regbn explaigs that all duty regard anima@s are indivect duty to one another from a philosophical point of viewl He dllustrates it with an doctrine_of_analogy involve children Ch&ldren for exemplar are unabl% to sign contracts and lack rights. But they are protecten by the moral contract nonetheless because of the sen&imental interests of others. So we have then dutirs involving these children duty regard them but no juties to them. Our du&ies in their cnse are indibect duty to other humln bein$s usually their parent*. 2 With this he supports the hypothesis that arimals muft be protect from suffer as it is moral to protect any live being from suff#ring not because we have a moral condense with them but mainly zue to respec@ of lifl and recognition of sufferdng @tself. one Claire Suddath A trief hkstory of Veganism time &0 Octob%r 20b8 two Tom Regan The case for a$imal rights 1989',\n",
       "  'positive': 'There is a great moral difference between humans and animals. Unlike animals, humans are capable of rational thought and can alter the world around them. Other creatures were put on this earth for mankind to use, and that includes eating meat. For all these reasons we say that men and women have rights and that animals don’t. This means that eating meat is in no way like murder. It is natural for human beings to farm, kill, and eat other species. In the wild there is a brutal struggle for existence. The fact that we humans have succeeded in that struggle by exploiting our natural environment means that we have a natural right over lower species. In fact farming animals is much less brutal than the pain and hardship that animals inflict on each other naturally in the wild.  Eating meat does not need to mean cruelty to animals. There are a growing number of organic and free-range farms that can provide meat without cruelty to animals. Similarly, it might be reasonable to argue for an extension of animal welfare laws to protect farm animals - but that does not mean that it is wrong in principle to eat meat.',\n",
       "  'negatives': ['Neither citizen nor subject, consumer nor customer: the supremacy of the individual  A sensible Libertarian position accepts the rights of people to do whatever they like as long as it doesn’t infringe upon the life of anyone else. That may sound like something that anyone could sign up to but the reality is not so simple. The Right may defend corporate greed and the Left government intervention but there is a clearer principle; I have the right not to have my air poisoned by your chemical company which means I don’t have to pay for any government body to clear up the mess.  The Oglala Sioux activist and actor, Russell Means has argued that “A libertarian society would not allow anyone to injure others by pollution because it insists on individual responsibility.”  All too often the line between consumer and citizen is blurred because the interest of both state and private actors have become conjoined leaving little or no room for the individual between them. A libertarian approach would break that cozy consensus.',\n",
       "   'It is difficult to see how the life of anyone is improved by reducing sex to a cheap form of entertainment. Certainly not the unborn children and not the objectified women. Proposition is more than happy for women to take control of their own fertility – indeed we would go further and suggest that their boyfriends and husbands should do so as well. Recreational sex, within wedlock and during times of infertility removes all of these problems; a little planning and restraint achieves that aim. It also means that both parents need to show that they are responsible for the results; Op seems happy to say that people are uncontrollable beasts with no control over their desires – hardly an edifying concept.',\n",
       "   'Government should focus on the most needy  A primary responsibility of the government is for reducing inequality and ensuring that everyone has a basic living standard. A basic living standard includes food. As a result providing breakfasts should be for those who are most in need of a helping hand from government. Those who are wealthier and can afford their own breakfast do not need this help so any such breakfast policy should be means tested to only apply to those who need it. This is the case with the United States School Breakfast Program.',\n",
       "   'Communities should have a say in what is taught in schools, and many communities want to teach creationism.  Society is made up of communities with their own views on politics, religion, education, etc. School boards should be able to set curriculum based on the desires of the public, not just on what the scientific elites command to be taught. Children deserve to hear that their beliefs and those of their community are respected in the classroom. This is why Creationism, a belief held to varying extents in many countries, should be taught in the classroom. This is particularly true in the United States, where in several states the majority of people does not accept evolution, but have instead adopted Creationism, considering the evidence for the latter to be more convincing. [1] In a poll in 2009 a majority (57%) said that creationism should be taught in schools either without evolution or alongside it. [2] The teaching of Creationism should not be taught exclusively, but should share time with other prevailing theories, particularly those of evolution and abiogenesis. Furthermore, evolution taught exclusively threatens religious belief, telling children they are no more than animals and lack the spark of grace given by God. It is important for social stability that schools are allowed to teach what communities believe to be true.  [1] Goodstein, Laurie. 2005. “Teaching of Creationism is Endorsed in New Survey”. New York Times.  [2] HarrisInteractive. 2009. “No Consensus, and Much Confusion, on Evolution and the Origin of Species.” BBC World News America/The Harris Poll, 18th February, 2009.',\n",
       "   'The Arctic is a diverse but fragile ecosystem  Mineral extraction is not a clean process [1] and the Arctic is acknowledged as a fragile ecosystem. In addition to the pollution that using these fuels will cause elsewhere in the world, the process of extraction itself is fraught with risks. There is some destruction caused simply by the process of building and running rigs with everything running normally, but the nightmare scenario is a major spill. [2] Let’s be clear, with the best will in the world, there will be a spill; difficult and unpredictable conditions, gruelling tests for both the machinery and the engineers that manage it, and a track record that leaves a lot to be desired in far more habitable and accessible environments. There are two difficulties posed in terms of an off-shore (or below-ice in this case) spill. The first problem is that stopping the spill would be vastly more complicated logistically than anything previously attempted, making previous deep-sea containment exercises seem simple by comparison. [3]  The Exxon Valdez disaster showed the large scale damage that oil spills near the poles can have large and long lasting effects on the ecosystem; hundreds of thousands of seabirds were killed in the spill and it is estimates some habitats will take 30 years to recover. [4] Any such disaster is made much worse above the arctic circle because of the cold. Oil degrades faster in warmer waters because the metabolism of microbes that break the oil down works much more slowly in the cold arctic waters, at the same time the oil spreads out less so provides less surface area. [5] In 2010 it was reported that more than two decades after the spill there were still 23,000 gallons of relatively un-weathered oil in Prince William Sound. [6]  The second issue, as demonstrated by large scale experimentation in the 1970s is that the oil would interact with the Polar ice to affect a far larger area than would normally be the case.  At the very least, it seems sensible to have a moratorium on sub-glacial drilling until the technology is available to deal safely and securely with a spill.  [1] Bibby, N. Is Norman Baker Serious about Saving the Environment? Liberalconspiracy.org. 18 march 2012.   [2] McCarthy, Michael, The Independent. Oil exploration under the arctic could cause ‘uncontrollable’ natural disaster. 6 September 201   [3] Vidal, John, ‘Why an oil spill in Arctic waters would be devastating’, The Guardian, 22 April 2011,   [4] Williamson, David, ‘Exxon Valdez oil spill impacts lasting far longer than expected, scientists say’, UNC News Services, 18 December 2003, no.648,   [5] Atlas, Ronald M., et al., ‘Microbes &amp; Oil Spills – FAQ’, Fisheries and Oceans Canada, 22 April 2013,   [6] National Oceanic and Atmospheric Administration, ‘Intent to Prepare a Supplemental Environmental Impact Statement on the Exxon Valdez Oil Spill Trustee Council’s Restoration Efforts’, Federal Register, Vol.75., No. 14, 22 January 2010, p.3707']}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_pairs[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ade92f9-e3df-4d0a-b849-b1a721e18cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1406"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b97f7b4-f007-41f6-bfbb-1b9b01665e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a415b29a-a543-4679-8f78-44f715b3067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDataset:\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.pairs[idx]\n",
    "        return item[\"anchor\"], item[\"positive\"], item[\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a39efeab-fd20-411a-80c5-315f362e00d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_dataset = ContrastiveDataset(contrastive_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d103bbfa-895a-4020-a7e5-21de659f27dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(contrastive_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8b0277f-f526-4087-82da-c4cdd65d32da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e15526a4-b3fe-435b-91c9-ad4064562f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1131255/3105893467.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_model = torch.load(file_path)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "file_path_baseline =\"/dss/dsshome1/07/ra65bex2/srawat/contrastive_learning/v1.1/app_baseline/checkpoint_epoch_3.pth\"\n",
    "lora_model_baseline = torch.load(file_path_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9999e628-5966-4dda-b5c5-c0d762fce27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "lora_model_baseline = lora_model_baseline.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e6a3a61-8db0-43db-b575-6399f8515950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68ac4919-76f1-4349-a314-4b1061126655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x, y):\n",
    "    return 1 - torch.nn.functional.cosine_similarity(x, y, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "990f63bd-4d48-4f58-a001-3b3250f65312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mrr_baseline(model, data_loader_val, distance_fn):\n",
    "    model.eval()\n",
    "\n",
    "    total_rr = 0.0\n",
    "    num_queries = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader_val:\n",
    "            anchor_text = batch[0]\n",
    "            positive_text = batch[1]\n",
    "            negative_texts = batch[2]\n",
    "\n",
    "            anchor_input = tokenizer(anchor_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            positive_input = tokenizer(positive_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "            anchor_embedding = model(**anchor_input).last_hidden_state[:, 0, :]\n",
    "            positive_embedding = model(**positive_input).last_hidden_state[:, 0, :]\n",
    "            negative_embedding = [model(**tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state[:, 0, :] for neg in negative_texts]\n",
    "\n",
    "            pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "            neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "            all_similarities=torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "\n",
    "            sorted_similarities, sorted_indices = torch.sort(all_similarities, dim=1, descending=True)\n",
    "\n",
    "            # Find the rank of the first relevant (positive) document\n",
    "            positive_rank = (sorted_indices == 0).nonzero(as_tuple=True)[1] + 1  # +1 to make rank 1-based\n",
    "            total_rr += torch.sum(1.0 / positive_rank.float()).item()  # Reciprocal rank\n",
    "            num_queries += len(positive_rank)\n",
    "\n",
    "    mrr = total_rr / num_queries\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "534a9ec5-4211-4cb6-81f1-fb8a30caff92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5887104554486053\n"
     ]
    }
   ],
   "source": [
    "mrr_validation_baseline = evaluate_mrr_baseline(lora_model_baseline, data_loader, cosine_distance)\n",
    "print(mrr_validation_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bbd65-41c9-4ac7-80ef-ba4a4fb02d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3dfd17-ee43-4c4e-9287-8ec6451bc062",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_average=\"/dss/dsshome1/07/ra65bex2/srawat/contrastive_learning/v1.1/app_average/average_checkpoint_epoch_3.pth\"\n",
    "lora_model_average = torch.load(file_path_average)\n",
    "lora_model_average = lora_model_average.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c10d3f-c53d-4222-aba4-0e763451baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_embedding(inputs, model):\n",
    "    input=model(**inputs)\n",
    "    input_last_hidden_state=input.last_hidden_state\n",
    "    input_attention_mask = inputs['attention_mask']\n",
    "    input_masked_embeddings = input_last_hidden_state * input_attention_mask.unsqueeze(-1)\n",
    "    input_sum_embeddings = torch.sum(input_masked_embeddings, dim=1)\n",
    "    input_token_counts = torch.sum(input_attention_mask, dim=1).unsqueeze(-1)\n",
    "    input_avg_embeddings = input_sum_embeddings / input_token_counts\n",
    "    return(input_avg_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22570a1b-93bf-41ef-99fe-de41c3857b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mrr_average(model, data_loader_val, distance_fn):\n",
    "    model.eval()  \n",
    "\n",
    "    total_rr = 0.0\n",
    "    num_queries = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader_val:\n",
    "            anchor_text = batch[0]\n",
    "            positive_text = batch[1]\n",
    "            negative_texts = batch[2]\n",
    "\n",
    "            anchor_input = tokenizer(anchor_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            positive_input = tokenizer(positive_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "            anchor_embedding = avg_embedding(anchor_input, model)\n",
    "            positive_embedding = avg_embedding(positive_input, model)\n",
    "            negative_embedding = [avg_embedding(tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device), model) for neg in negative_texts]\n",
    "\n",
    "            pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "            neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "            all_similarities=torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "            \n",
    "            sorted_similarities, sorted_indices = torch.sort(all_similarities, dim=1, descending=True)\n",
    "\n",
    "            # Find the rank of the first relevant (positive) document\n",
    "            positive_rank = (sorted_indices == 0).nonzero(as_tuple=True)[1] + 1  # +1 to make rank 1-based\n",
    "            total_rr += torch.sum(1.0 / positive_rank.float()).item()  # Reciprocal rank\n",
    "            num_queries += len(positive_rank)\n",
    "\n",
    "    mrr = total_rr / num_queries\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82977667-a984-467e-9eb6-9d6520ecb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_validation_average = evaluate_mrr_average(lora_model_average, data_loader, cosine_distance)\n",
    "print(mrr_validation_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c23c7-dc92-49e8-bb1f-1520c0ccd13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58281c-1786-4bda-9eec-0fa0f127d140",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_hyperbolic=\"/dss/dsshome1/07/ra65bex2/srawat/contrastive_learning/v1.1/0.1hyperbolic/hyperbolic_lora_checkpoint_epoch_3.pth\"\n",
    "lora_model_hyperbolic = torch.load(file_path_hyperbolic)\n",
    "lora_model_hyperbolic = lora_model_hyperbolic.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9507b-01e9-49d0-8327-760677bfcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorentzian_distance(x, y):\n",
    "    \n",
    "    dot_product = torch.sum(x * y, dim=-1)\n",
    "    norm_x = torch.norm(x, dim=-1)\n",
    "    norm_y = torch.norm(y, dim=-1)\n",
    "    \n",
    "    distance = torch.acosh(-dot_product + torch.sqrt((1 + norm_x**2) * (1 + norm_y**2)))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ecc53-8b64-4fee-bfbf-d3b85d441d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expm_o(v, c=1.0):\n",
    "    c = torch.tensor(c)\n",
    "    vspace = v\n",
    "    vnorm = torch.norm(v, p=2, dim=-1, keepdim=True)\n",
    "    xspace = torch.sinh(torch.sqrt(c) * vnorm) * vspace / (torch.sqrt(c) * vnorm)\n",
    "    batch_min = xspace.min(dim=1, keepdim=True).values\n",
    "    batch_max = xspace.max(dim=1, keepdim=True).values\n",
    "    xspace_scaled=(xspace - batch_min) / (batch_max - batch_min)\n",
    "    return xspace_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6639968-6b1d-4601-a69f-6e83b9863ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mrr_hyperbolic(model1, data_loader_val, distance_fn):\n",
    "    model1.eval()\n",
    "    \n",
    "    total_rr = 0.0\n",
    "    num_queries = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader_val:\n",
    "            anchor_text = batch[0]\n",
    "            positive_text = batch[1]\n",
    "            negative_texts = batch[2]\n",
    "\n",
    "            anchor_input = tokenizer(anchor_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            positive_input = tokenizer(positive_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "            anchor_embedding = expm_o(model1(**anchor_input).last_hidden_state[:, 0, :])\n",
    "            positive_embedding = expm_o(model1(**positive_input).last_hidden_state[:, 0, :])\n",
    "            negative_embedding = [expm_o(model1(**tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state[:, 0, :]) for neg in negative_texts]\n",
    "\n",
    "            pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "            neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "            all_similarities=torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "\n",
    "            sorted_similarities, sorted_indices = torch.sort(all_similarities, dim=1, descending=True)\n",
    "\n",
    "            # Find the rank of the first relevant (positive) document\n",
    "            positive_rank = (sorted_indices == 0).nonzero(as_tuple=True)[1] + 1  # +1 to make rank 1-based\n",
    "            total_rr += torch.sum(1.0 / positive_rank.float()).item()  # Reciprocal rank\n",
    "            num_queries += len(positive_rank)\n",
    "            \n",
    "    mrr = total_rr / num_queries\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54c77e-2dde-4c88-9eaf-439a409e845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_validation_hyperbolic = evaluate_mrr_hyperbolic(model1=lora_model_hyperbolic, data_loader_val=data_loader,distance_fn=lorentzian_distance)\n",
    "print(mrr_validation_hyperbolic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e097240-0b21-4a85-8959-2c74522b15a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d883371b-b6e4-40de-b212-93ed5fcb6061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "print(\"Median Query Length:\", statistics.median(query_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02b505-a8f9-480d-959e-effcbc2ed3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_validation_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10dccb-fbc7-448b-b3d9-4d7e045dafd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_validation_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c22c87-c20f-4a86-a8d3-d941340ea726",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_validation_hyperbolic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
