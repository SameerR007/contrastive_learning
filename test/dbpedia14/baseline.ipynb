{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'title', 'content'],\n",
       "        num_rows: 560000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'title', 'content'],\n",
       "        num_rows: 70000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"dbpedia_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'title', 'content'],\n",
       "    num_rows: 560000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = load_dataset(\"dbpedia_14\", split=\"train\")\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'title': 'E. D. Abbott Ltd',\n",
       " 'content': ' Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_TO_CATEGORY = {\n",
    "    0: \"Company\",\n",
    "    1: \"Educational Institution\",\n",
    "    2: \"Artist\",\n",
    "    3: \"Athlete\",\n",
    "    4: \"Office Holder\",\n",
    "    5: \"Mean Of Transportation\",\n",
    "    6: \"Building\",\n",
    "    7: \"Natural Place\",\n",
    "    8: \"Village\",\n",
    "    9: \"Animal\",\n",
    "    10: \"Plant\",\n",
    "    11: \"Album\",\n",
    "    12: \"Film\",\n",
    "    13: \"Written Work\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_samples = {}\n",
    "for sample in dataset_train:\n",
    "    category = LABEL_TO_CATEGORY[sample[\"label\"]]\n",
    "    if category not in category_to_samples:\n",
    "        category_to_samples[category] = []\n",
    "    category_to_samples[category].append(sample[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_negatives = {\n",
    "    category: [desc for cat, descriptions in category_to_samples.items() if cat != category for desc in descriptions]\n",
    "    for category in LABEL_TO_CATEGORY.values()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example, num_negatives=5):\n",
    "    category = LABEL_TO_CATEGORY.get(example[\"label\"], None)\n",
    "    \n",
    "    if category is None or category not in category_negatives:\n",
    "        return None  \n",
    "\n",
    "    query = f\"Tell me about {category.lower()}.\"\n",
    "    positive = example[\"content\"]\n",
    "\n",
    "    negatives = random.choices(category_negatives[category], k=num_negatives)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 560000/560000 [03:02<00:00, 3061.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_data_train = dataset_train.map(preprocess, remove_columns=dataset_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 560000/560000 [00:53<00:00, 10402.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_data_train = processed_data_train.filter(lambda x: x['query'] is not None and x['positive'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'positive', 'negatives'],\n",
       "    num_rows: 560000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Tell me about company.',\n",
       " 'positive': ' Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.',\n",
       " 'negatives': [\" Bowman's Castle also known as Nemacolin Castle was built in present-day Brownsville Pennsylvania at the western terminus of the Nemacolin's Trail on the east bank of the Monongahela river. It replaced a wooden trading post built near the site of Fort Burd the latter built by British colonists during the French and Indian War. Construction on the castle including addition of a crenellated tower continued through the Victorian era when it was considered an engineering marvel.\",\n",
       "  ' RFA Olwen (A122) was an Ol-class fast fleet tanker of the Royal Fleet Auxiliary.The lead ship of her class and launched in 1964 as RFA Olynthus the second ship to bear this name she was renamed Olwen in 1967 to avoid confusion with HMS Olympus. She was decommissioned in 1999 and laid up at Portsmouth before being broken up at Alang India in 2001.',\n",
       "  ' Free Software Magazine (also known as FSM and originally titled The Open Voice) is a website which produces a (generally bi-monthly) mostly free-content e-zine about free software.It was started in November 2004 by Australian Tony Mobily the former editor of TUX Magazine under the auspices of The Open Company Partners Inc. (based in the United States of America) and carried the subtitle The free magazine for the free software world.',\n",
       "  \" Mr. Natural is the Bee Gees' twelfth album (tenth worldwide) released in July 1974. It was the first Bee Gees release to be produced by Arif Mardin who was partially responsible for launching the group's later major success with the follow-up album Main Course. The album reached No. 178 on the Billboard 200. Mr. Natural was also the first album to feature drummer Dennis Bryon.\",\n",
       "  \" Dr. José Celso Barbosa (July 27 1857 – September 21 1921) was a physician sociologist and political leader of Puerto Rico. Known within Puerto Rico's New Progressive Party as The father of the Statehood for Puerto Rico movement Barbosa was among the first five Puerto Rican leaders appointed in 1900 to the Executive Cabinet under Governor Charles H. Allen in the first civilian government organized by the United States. He served in the Cabinet until 1917.\"]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m contrastive_pairs_train \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocessed_data_train\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositive\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpositive\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\arrow_dataset.py:2382\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2381\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m-> 2382\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem(\n\u001b[0;32m   2383\u001b[0m             i,\n\u001b[0;32m   2384\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "contrastive_pairs_train = []\n",
    "for item in processed_data_train:\n",
    "    query = item[\"query\"]\n",
    "    positive = item[\"positive\"]\n",
    "    negatives = item[\"negatives\"]\n",
    "    contrastive_pairs_train.append({\n",
    "        \"anchor\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anchor': 'Tell me about film.',\n",
       " 'positive': ' Rojo Amanecer (Red Dawn) is a 1989 Silver Ariel Award-winning Mexican film directed by Jorge Fons. It is a film about the Tlatelolco Massacre in the section of Tlatelolco in Mexico City in the evening of October 2 1968. It focuses on the day of a middle-class Mexican family living in one of the apartment buildings surrounding the Plaza de Tlatelolco and is based on testimonials from witnesses and victims. It stars Héctor Bonilla María Rojo the Bichir Brothers Eduardo Palomo and others.',\n",
       " 'negatives': [\" Annuska Johanna Maria 'Anouska' van der Zee (born April 5 1976 in Utrecht) is a retired Dutch racing cyclist. She participated both on track and at the road.Van der Zee represented the Netherlands at the 2004 Summer Olympics in Athens where she took part in the road race but did not reach the finish. After the Olympics she ended her career.\",\n",
       "  ' Mirrors is the second studio album from German singer Sandra released in 1986.',\n",
       "  \" La Cucaracha is the 10th and final studio album released by the alternative rock band Ween. The album was available streaming on the band's MySpace page October 16–19 2007 and it was released on October 23. It was named the 2007 Album of the Year by Magnet magazine. A bonus track Bag of Fat was available as an iTunes exclusive with the purchase of the entire album.\",\n",
       "  ' Alfred A. Knopf Inc. is a New York publishing house founded by Alfred A. Knopf Sr. in 1915. It was acquired by Random House in 1960 and is now part of the Knopf Doubleday Publishing Group at Random House which has been owned since 1998 by the German private media corporation Bertelsmann. The publishing house is known for its borzoi colophon (shown at right) which was designed by co-founder Blanche Knopf. Many of its hardcover books later appear as Vintage paperbacks.',\n",
       "  \" Wiregrass Commons Mall is an enclosed shopping mall located in Dothan Alabama. It has 638554 square feet (59000 m2) of shopping with over fifty retail stores and a food court. It is Southeast Alabama's only shopping mall. The mall's anchor tenants are Belk Burlington Coat Factory (opened 2009) Dillard's and JCPenney.\"]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive_pairs_train[500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contrastive_pairs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveDataset:\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.pairs[idx]\n",
    "        return item[\"anchor\"], item[\"positive\"], item[\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_dataset_train = ContrastiveDataset(contrastive_pairs_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(contrastive_dataset_train, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type= \"FEATURE_EXTRACTION\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(x, y):\n",
    "    return 1 - torch.nn.functional.cosine_similarity(x, y, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_nce_loss(anchor_embedding, positive_embedding, negative_embedding, distance_fn):\n",
    "\n",
    "    pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "    neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "    \n",
    "    logits = torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'title', 'content'],\n",
       "    num_rows: 70000\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val = load_dataset(\"dbpedia_14\", split=\"test\")\n",
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_to_samples_val = {}\n",
    "for sample in dataset_val:\n",
    "    category = LABEL_TO_CATEGORY[sample[\"label\"]]\n",
    "    if category not in category_to_samples_val:\n",
    "        category_to_samples_val[category] = []\n",
    "    category_to_samples_val[category].append(sample[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_negatives_val = {\n",
    "    category: [desc for cat, descriptions in category_to_samples_val.items() if cat != category for desc in descriptions]\n",
    "    for category in LABEL_TO_CATEGORY.values()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_val(example, num_negatives=5):\n",
    "    category = LABEL_TO_CATEGORY.get(example[\"label\"], None)\n",
    "    \n",
    "    if category is None or category not in category_negatives_val:\n",
    "        return None\n",
    "\n",
    "    query = f\"Tell me about {category.lower()}.\"\n",
    "    positive = example[\"content\"]\n",
    "\n",
    "    negatives = random.choices(category_negatives_val[category], k=num_negatives)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 70000/70000 [01:33<00:00, 750.35 examples/s] \n"
     ]
    }
   ],
   "source": [
    "processed_data_val = dataset_val.map(preprocess_val, remove_columns=dataset_val.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 70000/70000 [00:08<00:00, 7896.43 examples/s] \n"
     ]
    }
   ],
   "source": [
    "processed_data_val = processed_data_val.filter(lambda x: x['query'] is not None and x['positive'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_pairs_val = []\n",
    "for item in processed_data_val:\n",
    "    query = item[\"query\"]\n",
    "    positive = item[\"positive\"]\n",
    "    negatives = item[\"negatives\"]\n",
    "    contrastive_pairs_val.append({\n",
    "        \"anchor\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_dataset_val = ContrastiveDataset(contrastive_pairs_val[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_val = DataLoader(contrastive_dataset_val, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mrr(model, data_loader_val, distance_fn):\n",
    "    model.eval()\n",
    "\n",
    "    total_rr = 0.0\n",
    "    num_queries = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader_val:\n",
    "            anchor_text = batch[0]\n",
    "            positive_text = batch[1]\n",
    "            negative_texts = batch[2]\n",
    "\n",
    "            anchor_input = tokenizer(anchor_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            positive_input = tokenizer(positive_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "            anchor_embedding = model(**anchor_input).last_hidden_state[:, 0, :]\n",
    "            positive_embedding = model(**positive_input).last_hidden_state[:, 0, :]\n",
    "            negative_embedding = [model(**tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state[:, 0, :] for neg in negative_texts]\n",
    "\n",
    "            pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "            neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "            all_similarities=torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "\n",
    "            sorted_similarities, sorted_indices = torch.sort(all_similarities, dim=1, descending=True)\n",
    "\n",
    "            # Find the rank of the first relevant (positive) document\n",
    "            positive_rank = (sorted_indices == 0).nonzero(as_tuple=True)[1] + 1  # +1 to make rank 1-based\n",
    "            total_rr += torch.sum(1.0 / positive_rank.float()).item()  # Reciprocal rank\n",
    "            num_queries += len(positive_rank)\n",
    "\n",
    "    mrr = total_rr / num_queries\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir =\"/dss/dsshome1/07/ra65bex2/srawat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epoch_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    lora_model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader_train:\n",
    "    \n",
    "        anchor_texts = batch[0]\n",
    "        positive_texts = batch[1]\n",
    "        negative_texts = batch[2]\n",
    "    \n",
    "        anchor_inputs = tokenizer(anchor_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        positive_inputs = tokenizer(positive_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "    \n",
    "        anchor_embedding = lora_model(**anchor_inputs).last_hidden_state[:, 0, :]\n",
    "        positive_embedding = lora_model(**positive_inputs).last_hidden_state[:, 0, :]\n",
    "        negative_embedding = [lora_model(**tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state[:, 0, :] for neg in negative_texts]\n",
    "\n",
    "        loss = info_nce_loss(anchor_embedding, positive_embedding, negative_embedding, distance_fn=cosine_distance)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    save_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    torch.save(lora_model, save_path)\n",
    "    print(f\"EPOCH {epoch+1}:\")\n",
    "    print(f\"Checkpoint saved: {save_path}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss / len(data_loader_train)}\")\n",
    "    mrr_validation = evaluate_mrr(lora_model, data_loader_val, cosine_distance)\n",
    "    #mrr_train = evaluate_mrr(lora_model, data_loader_train, cosine_distance)\n",
    "    #print(f\"Mean Reciprocal Rank (MRR) for training set: {mrr_train:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR) for validation set: {mrr_validation:.4f}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1} took {(end_time - start_time) / 60:.4f} minutes.\")\n",
    "    print(f\"\\n\")\n",
    "    epoch_metrics.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'training_loss': total_loss / len(data_loader_train),\n",
    "        'mrr_validation': mrr_validation,\n",
    "        'time_taken_minutes': (end_time - start_time) / 60\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_dir + '/epoch_metrics.json', 'w') as f:\n",
    "    json.dump(epoch_metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
