{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c643a105",
   "metadata": {
    "papermill": {
     "duration": 6.998238,
     "end_time": "2025-02-03T01:37:35.734856",
     "exception": false,
     "start_time": "2025-02-03T01:37:28.736618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bcba0194",
   "metadata": {
    "papermill": {
     "duration": 2.374298,
     "end_time": "2025-02-03T01:37:38.118381",
     "exception": false,
     "start_time": "2025-02-03T01:37:35.744083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"CShorten/ML-ArXiv-Papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9c77ff31",
   "metadata": {
    "papermill": {
     "duration": 0.011238,
     "end_time": "2025-02-03T01:37:38.140062",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.128824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'title', 'abstract'],\n",
       "        num_rows: 117592\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "04a2a273",
   "metadata": {
    "papermill": {
     "duration": 0.047292,
     "end_time": "2025-02-03T01:37:38.192133",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.144841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_datasets = dataset[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9dbe1eb6",
   "metadata": {
    "papermill": {
     "duration": 0.009377,
     "end_time": "2025-02-03T01:37:38.206374",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.196997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'title', 'abstract'],\n",
       "        num_rows: 94073\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0.1', 'Unnamed: 0', 'title', 'abstract'],\n",
       "        num_rows: 23519\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e518138",
   "metadata": {
    "papermill": {
     "duration": 0.008713,
     "end_time": "2025-02-03T01:37:38.219967",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.211254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = split_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0f892510",
   "metadata": {
    "papermill": {
     "duration": 0.009058,
     "end_time": "2025-02-03T01:37:38.233944",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.224886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0.1', 'Unnamed: 0', 'title', 'abstract'],\n",
       "    num_rows: 94073\n",
       "})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "135d2875",
   "metadata": {
    "papermill": {
     "duration": 0.009902,
     "end_time": "2025-02-03T01:37:38.248859",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.238957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.remove_columns(['Unnamed: 0','Unnamed: 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cbcdbc4c",
   "metadata": {
    "papermill": {
     "duration": 0.009293,
     "end_time": "2025-02-03T01:37:38.263143",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.253850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'abstract'],\n",
       "    num_rows: 94073\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1c3dbc53",
   "metadata": {
    "papermill": {
     "duration": 0.010085,
     "end_time": "2025-02-03T01:37:38.278239",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.268154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Predicting Visual Overlap of Images Through Interpretable Non-Metric Box\\n  Embeddings',\n",
       " 'abstract': \"  To what extent are two images picturing the same 3D surfaces? Even when this\\nis a known scene, the answer typically requires an expensive search across\\nscale space, with matching and geometric verification of large sets of local\\nfeatures. This expense is further multiplied when a query image is evaluated\\nagainst a gallery, e.g. in visual relocalization. While we don't obviate the\\nneed for geometric verification, we propose an interpretable image-embedding\\nthat cuts the search in scale space to essentially a lookup.\\n  Our approach measures the asymmetric relation between two images. The model\\nthen learns a scene-specific measure of similarity, from training examples with\\nknown 3D visible-surface overlaps. The result is that we can quickly identify,\\nfor example, which test image is a close-up version of another, and by what\\nscale factor. Subsequently, local features need only be detected at that scale.\\nWe validate our scene-specific model by showing how this embedding yields\\ncompetitive image-matching results, while being simpler, faster, and also\\ninterpretable by humans.\\n\"}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "045e63c7",
   "metadata": {
    "papermill": {
     "duration": 5.109443,
     "end_time": "2025-02-03T01:37:43.392763",
     "exception": false,
     "start_time": "2025-02-03T01:37:38.283320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2754c60a0b44468abf00dd2401b0064e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = dataset_train.map(lambda x, idx: { 'index': idx }, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f4e9748a",
   "metadata": {
    "papermill": {
     "duration": 0.009764,
     "end_time": "2025-02-03T01:37:43.410221",
     "exception": false,
     "start_time": "2025-02-03T01:37:43.400457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'abstract', 'index'],\n",
       "    num_rows: 94073\n",
       "})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "69efe8a2",
   "metadata": {
    "papermill": {
     "duration": 0.010187,
     "end_time": "2025-02-03T01:37:43.425796",
     "exception": false,
     "start_time": "2025-02-03T01:37:43.415609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'A Probabilistic Framework to Node-level Anomaly Detection in\\n  Communication Networks',\n",
       " 'abstract': '  In this paper we consider the task of detecting abnormal communication volume\\noccurring at node-level in communication networks. The signal of the\\ncommunication activity is modeled by means of a clique stream: each occurring\\ncommunication event is instantaneous and activates an undirected subgraph\\nspanning over a set of equally participating nodes. We present a probabilistic\\nframework to model and assess the communication volume observed at any single\\nnode. Specifically, we employ non-parametric regression to learn the\\nprobability that a node takes part in a certain event knowing the set of other\\nnodes that are involved. On the top of that, we present a concentration\\ninequality around the estimated volume of events in which a node could\\nparticipate, which in turn allows us to build an efficient and interpretable\\nanomaly scoring function. Finally, the superior performance of the proposed\\napproach is empirically demonstrated in real-world sensor network data, as well\\nas using synthetic communication activity that is in accordance with that\\nlatter setting.\\n',\n",
       " 'index': 4}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "63f1c92e",
   "metadata": {
    "papermill": {
     "duration": 0.010049,
     "end_time": "2025-02-03T01:37:43.441339",
     "exception": false,
     "start_time": "2025-02-03T01:37:43.431290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning',\n",
       " 'abstract': '  Multi-hop QA requires a model to connect multiple pieces of evidence\\nscattered in a long context to answer the question. The recently proposed\\nHotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four\\ndifferent multi-hop reasoning paradigms (two bridge entity setups, checking\\nmultiple properties, and comparing two entities), making it challenging for a\\nsingle neural network to handle all four. In this work, we present an\\ninterpretable, controller-based Self-Assembling Neural Modular Network (Hu et\\nal., 2017, 2018) for multi-hop reasoning, where we design four novel modules\\n(Find, Relocate, Compare, NoOp) to perform unique types of language reasoning.\\nBased on a question, our layout controller RNN dynamically infers a series of\\nreasoning modules to construct the entire network. Empirically, we show that\\nour dynamic, multi-hop modular network achieves significant improvements over\\nthe static, single-hop baseline (on both regular and adversarial evaluation).\\nWe further demonstrate the interpretability of our model via three analyses.\\nFirst, the controller can softly decompose the multi-hop question into multiple\\nsingle-hop sub-questions to promote compositional reasoning behavior of the\\nmain network. Second, the controller can predict layouts that conform to the\\nlayouts designed by human experts. Finally, the intermediate module can infer\\nthe entity that connects two distantly-located supporting facts by addressing\\nthe sub-question from the controller.\\n',\n",
       " 'index': 20572}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train[20572]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e9993947",
   "metadata": {
    "papermill": {
     "duration": 0.055716,
     "end_time": "2025-02-03T01:37:43.502650",
     "exception": false,
     "start_time": "2025-02-03T01:37:43.446934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_indexes = np.array(dataset_train['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3c956f8e",
   "metadata": {
    "papermill": {
     "duration": 0.011072,
     "end_time": "2025-02-03T01:37:43.519321",
     "exception": false,
     "start_time": "2025-02-03T01:37:43.508249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_hard_negatives(example, num_negatives=10,dataset=dataset_train):\n",
    "    query_index = example['index']  \n",
    "    anchor = example['title']\n",
    "    positive = example['abstract']\n",
    "    \n",
    "    negatives = []\n",
    "    \n",
    "    negative_indexes = np.delete(all_indexes, np.where(all_indexes == query_index))\n",
    "    \n",
    "    sampled_negatives = random.sample(list(negative_indexes), num_negatives)\n",
    "    \n",
    "    for idx in sampled_negatives:\n",
    "        negatives.append(dataset[int(idx)]['abstract'])\n",
    "    \n",
    "    return {\n",
    "        \"query\": anchor,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab12866",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-02-03T01:37:43.525186",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3df2653718c43ee80362ee6d4bd5b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94073 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_data_train = dataset_train.map(generate_hard_negatives, remove_columns=dataset_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c271d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96368bc5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188e153",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contrastive_pairs_train = []\n",
    "for item in processed_data_train:\n",
    "    query = item[\"query\"]\n",
    "    positive = item[\"positive\"]\n",
    "    negatives = item[\"negatives\"]\n",
    "    contrastive_pairs_train.append({\n",
    "        \"anchor\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c6a2e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(contrastive_pairs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670559eb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927b2173",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContrastiveDataset:\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.pairs[idx]\n",
    "        return item[\"anchor\"], item[\"positive\"], item[\"negatives\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6918b6f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contrastive_dataset_train = ContrastiveDataset(contrastive_pairs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362b05af",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader_train = DataLoader(contrastive_dataset_train, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89b105",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(data_loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec267846",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfef80f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191c4f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5386cc4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type= \"FEATURE_EXTRACTION\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411c521",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81fc5d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cosine_distance(x, y):\n",
    "    return 1 - torch.nn.functional.cosine_similarity(x, y, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6dc08",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def info_nce_loss(anchor_embedding, positive_embedding, negative_embedding, distance_fn):\n",
    "\n",
    "    pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "    neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "    \n",
    "    logits = torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()(logits, labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ff980",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997d7dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_epochs=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93312d1a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be00cde7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5333e18",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d718a7f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_model = lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b1bf1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_val = split_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca99f37",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314d3d6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_val = dataset_val.remove_columns(['Unnamed: 0','Unnamed: 0.1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b2f1a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_val = dataset_val.map(lambda x, idx: { 'index': idx }, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454fc03a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59611a3b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_indexes_val = np.array(dataset_val['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a8e7e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_hard_negatives_val(example, num_negatives=10,dataset=dataset_val):\n",
    "    query_index = example['index'] \n",
    "    anchor = example['title']\n",
    "    positive = example['abstract']\n",
    "    \n",
    "    negatives = []\n",
    "    \n",
    "    negative_indexes = np.delete(all_indexes_val, np.where(all_indexes_val == query_index)) \n",
    "    \n",
    "    sampled_negatives = random.sample(list(negative_indexes), num_negatives)\n",
    "    \n",
    "    for idx in sampled_negatives:\n",
    "        negatives.append(dataset[int(idx)]['abstract'])\n",
    "    \n",
    "    return {\n",
    "        \"query\": anchor,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956100e1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "processed_data_val = dataset_val.map(generate_hard_negatives_val, remove_columns=dataset_val.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ecac28",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contrastive_pairs_val = []\n",
    "for item in processed_data_val:\n",
    "    query = item[\"query\"]\n",
    "    positive = item[\"positive\"]\n",
    "    negatives = item[\"negatives\"]\n",
    "    contrastive_pairs_val.append({\n",
    "        \"anchor\": query,\n",
    "        \"positive\": positive,\n",
    "        \"negatives\": negatives\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce529ab6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contrastive_dataset_val = ContrastiveDataset(contrastive_pairs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac55437",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader_val = DataLoader(contrastive_dataset_val, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73033c1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(data_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c29dae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_mrr(model, data_loader_val, distance_fn):\n",
    "    model.eval()\n",
    "\n",
    "    total_rr = 0.0\n",
    "    num_queries = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader_val:\n",
    "            anchor_text = batch[0]\n",
    "            positive_text = batch[1]\n",
    "            negative_texts = batch[2]\n",
    "\n",
    "            anchor_input = tokenizer(anchor_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "            positive_input = tokenizer(positive_text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "            anchor_embedding = model(**anchor_input).last_hidden_state[:, 0, :]\n",
    "            positive_embedding = model(**positive_input).last_hidden_state[:, 0, :]\n",
    "            negative_embedding = [model(**tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state[:, 0, :] for neg in negative_texts]\n",
    "\n",
    "            pos_dist = distance_fn(anchor_embedding, positive_embedding)\n",
    "            neg_dist = torch.stack([distance_fn(anchor_embedding, neg) for neg in negative_embedding], dim=-1)\n",
    "            all_similarities=torch.cat([-pos_dist.unsqueeze(1), -neg_dist], dim=1)\n",
    "\n",
    "            sorted_similarities, sorted_indices = torch.sort(all_similarities, dim=1, descending=True)\n",
    "\n",
    "            # Find the rank of the first relevant (positive) document\n",
    "            positive_rank = (sorted_indices == 0).nonzero(as_tuple=True)[1] + 1  # +1 to make rank 1-based\n",
    "            total_rr += torch.sum(1.0 / positive_rank.float()).item()  # Reciprocal rank\n",
    "            num_queries += len(positive_rank)\n",
    "\n",
    "    mrr = total_rr / num_queries\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84893f41",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "save_dir =\"/dss/dsshome1/07/ra65bex2/srawat/baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e46bee6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "epoch_metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f95333",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a21d84",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    lora_model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    for batch in data_loader_train:\n",
    "    \n",
    "        anchor_texts = batch[0]\n",
    "        positive_texts = batch[1]\n",
    "        negative_texts = batch[2]\n",
    "    \n",
    "        anchor_inputs = tokenizer(anchor_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "        positive_inputs = tokenizer(positive_texts, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
    "\n",
    "    \n",
    "        anchor_embedding = lora_model(**anchor_inputs).last_hidden_state[:, 0, :]\n",
    "        positive_embedding = lora_model(**positive_inputs).last_hidden_state[:, 0, :]\n",
    "        negative_embedding = [lora_model(**tokenizer(neg, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)).last_hidden_state[:, 0, :] for neg in negative_texts]\n",
    "\n",
    "        loss = info_nce_loss(anchor_embedding, positive_embedding, negative_embedding, distance_fn=cosine_distance)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    save_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "    torch.save(lora_model, save_path)\n",
    "    print(f\"EPOCH {epoch+1}:\")\n",
    "    print(f\"Checkpoint saved: {save_path}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss / len(data_loader_train)}\")\n",
    "    mrr_validation = evaluate_mrr(lora_model, data_loader_val, cosine_distance)\n",
    "    #mrr_train = evaluate_mrr(lora_model, data_loader_train, cosine_distance)\n",
    "    #print(f\"Mean Reciprocal Rank (MRR) for training set: {mrr_train:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR) for validation set: {mrr_validation:.4f}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Epoch {epoch+1} took {(end_time - start_time) / 60:.4f} minutes.\")\n",
    "    print(f\"\\n\")\n",
    "    epoch_metrics.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'training_loss': total_loss / len(data_loader_train),\n",
    "        'mrr_validation': mrr_validation,\n",
    "        'time_taken_minutes': (end_time - start_time) / 60\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c27c8a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a682b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(save_dir + '/epoch_metrics.json', 'w') as f:\n",
    "    json.dump(epoch_metrics, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7021e82",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "baseline.ipynb",
   "output_path": "baseline.ipynb",
   "parameters": {},
   "start_time": "2025-02-03T01:37:23.753984",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
